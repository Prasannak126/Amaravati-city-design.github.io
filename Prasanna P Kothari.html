<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prasanna P Kothari</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
            padding: 20px;
        }
        h1, h2, h3 {
            color: #333;
        }
        ul, ol {
            margin-left: 20px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        table, th, td {
            border: 1px solid #ddd;
        }
        th, td {
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #f4f4f4;
        }
        .highlight {
            background-color: #e6f7ff;
        }
    </style>
</head>
<body>
    <h1>Prasanna P Kothari Overview</h1>

    <!-- Section 1: Problems in Nature -->
    <h2>1. Problems in Nature</h2>
    <ul>
        <li><strong>Recursion:</strong> Uncontrolled recursion, like in cancer, can lead to exponential growth.</li>
        <li><strong>Iteration:</strong> Iterative cycles can be disrupted by climate change, leading to irregular seasons or extreme weather patterns.</li>
        <li><strong>Backtracking:</strong> Migratory species may encounter obstacles and fail to backtrack to alternative routes.</li>
    </ul>

    <!-- Section 2: Space and Time Efficiency -->
    <h2>2. Space and Time Efficiency</h2>
    <ul>
        <li><strong>Space Efficiency:</strong> Extra space required by an algorithm.</li>
        <li><strong>Time Efficiency:</strong> Extra time required by an algorithm.</li>
    </ul>
    <p><strong>Why Important?</strong> Space and time efficiency are vital for building systems that are fast, scalable, and sustainable, ensuring optimal performance across real-world applications.</p>

    <!--  Orders of Growth -->
    <h2>3. Orders of Growth</h2>
    <table>
        <thead>
            <tr>
                <th>Order</th>
                <th>Growth</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>O(1)</td><td>Constant</td></tr>
            <tr><td>O(n)</td><td>Linear</td></tr>
            <tr><td>O(log n)</td><td>Logarithmic</td></tr>
            <tr><td>O(n^2)</td><td>Quadratic</td></tr>
            <tr><td>O(n log n)</td><td>Linearithmic</td></tr>
            <tr><td>O(n^3)</td><td>Cubic</td></tr>
            <tr><td>O(n^k)</td><td>Polynomial</td></tr>
            <tr><td>O(2^n)</td><td>Exponential</td></tr>
            <tr><td>O(n!)</td><td>Factorial</td></tr>
        </tbody>
    </table>

    <!-- Section 4: Takeaway from Different Design Principles -->
    <h2>3. Takeaway from Different Design Principles</h2>
    <ul>
        <li><strong>Sorting Algorithms:</strong></li>
        <ul>
            <li><strong>Bubble Sort:</strong> Simple to understand and implement, best for small datasets.</li>
            <li><strong>Selection Sort:</strong> Minimal swaps compared to bubble sort but not adaptive.</li>
            <li><strong>Insertion Sort:</strong> Efficient for partially sorted datasets.</li>
            <li><strong>Merge Sort:</strong> Stable and efficient for large dataset sorting.</li>
            <li><strong>Quick Sort:</strong> In-place but not stable, efficient for large datasets.</li>
            <li><strong>Heap Sort:</strong> In-place but not stable.</li>
        </ul>
        <li><strong>Advanced Algorithms:</strong></li>
        <ul>
            <li><strong>Boyer-Moore Algorithm:</strong> Effective for long texts and patterns, utilizes pre-computation for optimal performance but requires extra space for preprocessing.</li>
            <li><strong>Kruskal’s Algorithm:</strong> Requires cycle detection, often implemented using union-find data structure. Suitable for sparse graphs.</li>
            <li><strong>Dijkstra’s Algorithm:</strong> Provides optimal solutions for single-source shortest paths in directed or undirected graphs.</li>
            <li><strong>Floyd’s Algorithm:</strong> All pair shortest path algorithm.</li>
            <li><strong>Warshall’s Algorithm:</strong> Based on the principle of Kleene’s theorem, checks transitive property for connectivity.</li>
            <li><strong>Prim’s Algorithm:</strong> Builds a minimum spanning tree using edge relaxation principles, falls under greedy techniques.</li>
        </ul>
    </ul>

    <!-- Section 4: Hierarchical Data and Tree Data Structures -->
    <h2>5. Hierarchical Data and Tree Data Structures</h2>
    <ul>
        <li><strong>Tree:</strong> Flexible but search or traversal may take O(n) due to lack of ordering or balancing.</li>
        <li><strong>BST (Binary Search Tree):</strong> Reduces the complexity of search operations to O(log n) on average by maintaining sorted order.</li>
        <li><strong>AVL Tree:</strong> Balances the tree after every insertion or deletion, ensuring O(log n) operations.</li>
        <li><strong>2-3 Tree:</strong> Always balanced, handling multiple keys in each node.</li>
        <li><strong>Red-Black Tree:</strong> Less strict balancing, reducing the overhead of rotations during updates.</li>
        <li><strong>Heap:</strong> Guarantees O(1) for finding max/min and O(log n) for insertions and deletions.</li>
        <li><strong>Trie:</strong> Exploits shared prefixes, ideal for large datasets with overlapping entries.</li>
    </ul>

    <!-- Section 5: Array Query Algorithms -->
    <h2>6. Array Query Algorithms</h2>
    <ul>
        <li><strong>Fenwick Tree:</strong> Efficient range queries and point updates.</li>
        <li><strong>Segment Tree:</strong> Divides the array into segments for range queries and updates.</li>
        <li><strong>Lookup Table:</strong> Precomputes results for all possible inputs.</li>
    </ul>

    <!-- Section 6: Differentiation Between Trees and Graphs -->
    <h2>7. Differentiation Between Trees and Graphs</h2>
    <table>
        <thead>
            <tr>
                <th>Feature</th>
                <th>Tree</th>
                <th>Graph</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Definition</td>
                <td>A hierarchical data structure with nodes connected by edges.</td>
                <td>A general data structure with nodes (vertices) connected by edges (can be directed/undirected).</td>
            </tr>
            <tr>
                <td>Structure</td>
                <td>Connected and acyclic.</td>
                <td>May be connected or disconnected, cyclic or acyclic.</td>
            </tr>
            <tr>
                <td>Root</td>
                <td>Has a single root node.</td>
                <td>No concept of a root (unless it's a tree-based graph).</td>
            </tr>
            <tr>
                <td>Parent-Child Relationship</td>
                <td>Nodes follow a strict parent-child hierarchy.</td>
                <td>No strict hierarchy; any node can connect to any other node.</td>
            </tr>
            <tr>
                <td>Edges</td>
                <td>Exactly n−1 edges for n nodes.</td>
                <td>Can have any number of edges (up to n(n−1)/2 for undirected graphs).</td>
            </tr>
            <tr>
                <td>Cyclic Nature</td>
                <td>Always acyclic.</td>
                <td>Can contain cycles (e.g., directed graphs with loops).</td>
            </tr>
        </tbody>
    </table>

    <!-- : Tree Traversals -->
    <h2>8. Tree Traversals</h2>
    <ul>
        <li><strong>Preorder:</strong> Visit root → Left subtree → Right subtree.</li>
        <li><strong>Inorder:</strong> Visit Left subtree → Root → Right subtree (used in BSTs for sorted order).</li>
        <li><strong>Postorder:</strong> Visit Left subtree → Right subtree → Root.</li>
        <li><strong>Level-order:</strong> Breadth-first traversal level by level.</li>
    </ul>

    <!-- : Graph Traversals -->
    <h2>9. Graph Traversals</h2>
    <ul>
        <li><strong>Depth First Search (DFS)</strong></li>
        <li><strong>Breadth First Search (BFS)</strong></li>
    </ul>

    <!-- Section 7: Sorting and Searching Algorithms -->
    <h2>10. Sorting and Searching Algorithms</h2>
    <table>
        <thead>
            <tr>
                <th>Algorithm</th>
                <th>Technique</th>
                <th>Time Complexity</th>
                <th>Real-World Applications</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>Bubble Sort</td><td>Compare adjacent elements and swap if needed.</td><td>O(n^2)</td><td>Educational purposes to teach algorithm basics.</td></tr>
            <tr><td>Selection Sort</td><td>Find the minimum and place it at the beginning.</td><td>O(n^2)</td><td>Simple applications where memory is minimal (e.g., small embedded systems).</td></tr>
            <tr><td>Insertion Sort</td><td>Build a sorted portion by inserting elements.</td><td>O(n^2)</td><td>Small datasets like sorting playing cards.</td></tr>
            <tr><td>Merge Sort</td><td>Divide-and-conquer: split, sort, and merge.</td><td>O(n log n)</td><td>Sorting large datasets in external storage, like disk drives.</td></tr>
            <tr><td>Quick Sort</td><td>Partitioning based on a pivot element.</td><td>O(n log n)</td><td>Databases, language libraries (e.g., Python's Timsort).</td></tr>
            <tr><td>Heap Sort</td><td>Use a heap data structure to extract max/min.</td><td>O(n log n)</td><td>Scheduling systems and prioritization.</td></tr>
        </tbody>
    </table>

    <!-- Section 8: Importance of Graph Algorithms -->
    <h2>11. Importance of Graph Algorithms</h2>
    <ul>
        <li><strong>Dijkstra’s Algorithm:</strong> Finds the shortest path from a single source to all other vertices in a weighted graph with non-negative weights. Applications: Roadmaps, telecommunications.</li>
        <li><strong>Floyd’s Algorithm:</strong> Computes shortest paths between all pairs of vertices in a weighted graph. Applications: Urban transit planning.</li>
        <li><strong>Warshall’s Algorithm:</strong> Determines transitive closure to verify connectivity before spanning tree construction.</li>
        <li><strong>Kruskal’s Algorithm:</strong> Constructs a minimum spanning tree by sorting and adding edges in order of increasing weight. Applications: Power grids, telecommunication lines.</li>
        <li><strong>Prim’s Algorithm:</strong> Builds a minimum spanning tree by growing one vertex at a time. Efficient for dense graphs.</li>
    </ul>

    <!-- Section 9: Algorithm Design Techniques -->
    <h2>12. Algorithm Design Techniques</h2>
    <ul>
        <li><strong>Divide and Conquer:</strong> Split the problem into smaller subproblems, solve, and merge the solutions. Examples: Merge Sort, Quick Sort, Binary Search.</li>
        <li><strong>Greedy Algorithm:</strong> Make locally optimal choices at each step. Examples: Prim’s and Kruskal’s Algorithms.</li>
        <li><strong>Backtracking:</strong> Explore all possible solutions incrementally and abandon paths that fail constraints. Examples: Sudoku, N-Queens Problem.</li>
        <li><strong>Recursion:</strong> Solve a problem by solving smaller instances of the same problem. Examples: Tower of Hanoi, Fibonacci numbers.</li>
        <li><strong>Brute Force:</strong> Try all possible solutions and select the best one.</li>
    </ul>

</body>
</html>
