</head>
<body>
    <div class="content">
        <h1>Algorithm Concepts and Principles</h1>

        <h2>1. Problems in Nature</h2>
        <ul>
            <li><strong>Recursion:</strong> Uncontrolled recursion, like in cancer, can lead to exponential growth.</li>
            <li><strong>Iteration:</strong> Iterative cycles can be disrupted by climate change, leading to irregular seasons or extreme weather patterns.</li>
            <li><strong>Backtracking:</strong> Migratory species may encounter obstacles and fail to backtrack to alternative routes.</li>
        </ul>

        <h2>2. Space and Time Efficiency</h2>
        <p><strong>Space Efficiency:</strong> Extra space required by an algorithm.</p>
        <p><strong>Time Efficiency:</strong> Extra time required by an algorithm.</p>
        <p><strong>Why Important?</strong> Space and time efficiency are vital for building systems that are fast, scalable, and sustainable, ensuring optimal performance across real-world applications.</p>

        <h3>Orders of Growth</h3>
        <table>
            <tr>
                <th>Order</th>
                <th>Description</th>
            </tr>
            <tr><td>O(1)</td><td>Constant</td></tr>
            <tr><td>O(n)</td><td>Linear</td></tr>
            <tr><td>O(log n)</td><td>Logarithmic</td></tr>
            <tr><td>O(n²)</td><td>Quadratic</td></tr>
            <tr><td>O(n log n)</td><td>Linearithmic</td></tr>
            <tr><td>O(n³)</td><td>Cubic</td></tr>
            <tr><td>O(n^k)</td><td>Polynomial</td></tr>
            <tr><td>O(2^n)</td><td>Exponential</td></tr>
            <tr><td>O(n!)</td><td>Factorial</td></tr>
        </table>

        <h2>3. Takeaway from Different Design Principles</h2>
        <h3>Sorting Algorithms</h3>
        <ul>
            <li><strong>Bubble Sort:</strong> Simple to understand and implement, best for small datasets.</li>
            <li><strong>Selection Sort:</strong> Minimal swaps compared to bubble sort but not adaptive.</li>
            <li><strong>Insertion Sort:</strong> Efficient for partially sorted datasets.</li>
            <li><strong>Merge Sort:</strong> Stable and efficient for large dataset sorting.</li>
            <li><strong>Quick Sort:</strong> In-place but not stable, efficient for large datasets.</li>
            <li><strong>Heap Sort:</strong> In-place but not stable.</li>
            <li><strong>Boyer-Moore Algorithm:</strong> Effective for long texts and patterns. Utilizes pre-computation for optimal performance but requires extra space for preprocessing.</li>
            <li><strong>Kruskal’s Algorithm:</strong> Requires cycle detection, often implemented using union-find data structure. Suitable for sparse graphs.</li>
            <li><strong>Dijkstra’s Algorithm:</strong> Provides optimal solutions for single-source shortest paths in directed or undirected graphs.</li>
            <li><strong>Floyd’s Algorithm:</strong> Based on the principle of Kleene’s Theorem. All pair shortest path algorithm.</li>
            <li><strong>Warshall’s Algorithm:</strong> Based on the principle of Kleene’s Theorem. Transitive property algorithm. We can try to make the algorithm run faster by treating matrix as bit strings and employing bitwise operations.</li>
            <li><strong>Prim’s Algorithm:</strong> Based on edge relaxation principle. Minimum spanning tree algorithm. Falls under the Greedy technique.</li>
        </ul>

        <h2>4. Hierarchical Data and Tree Data Structures</h2>
        <ul>
            <li><strong>Tree:</strong> Flexible but search or traversal may take O(n) due to lack of ordering or balancing.</li>
            <li><strong>BST:</strong> Reduces the complexity of search operations to O(log n) on average by maintaining sorted order.</li>
            <li><strong>AVL Tree:</strong> Balances the tree after every insertion or deletion, ensuring O(log n) operations.</li>
            <li><strong>2-3 Tree:</strong> Always balanced, handling multiple keys in each node.</li>
            <li><strong>Red-Black Tree:</strong> Less strict balancing, reducing the overhead of rotations during updates.</li>
            <li><strong>Heap:</strong> Guarantees O(1) for finding max/min and O(log n) for insertions and deletions.</li>
            <li><strong>Trie:</strong> Exploits shared prefixes, ideal for large datasets with overlapping entries.</li>
        </ul>

        <h2>5. Array Query Algorithms</h2>
        <p>Array query algorithms: Fenwick Tree, Segment Tree, Lookup Table. These algorithms are essential for efficiently processing, retrieving, and analyzing data stored in arrays, especially with large datasets or frequently repeated queries.</p>
        <ul>
            <li><strong>Fenwick Tree:</strong> Supports efficient range queries and point updates. Application: Cumulative frequency counts in online queries.</li>
            <li><strong>Segment Tree:</strong> Divides array into segments for range queries and updates. Application: Dynamic histograms.</li>
            <li><strong>Lookup Table:</strong> Precomputes results for all possible inputs. Application: Collision detection, AI decision trees.</li>
        </ul>

        <h2>6. Differentiation Between Trees and Graphs</h2>
        <table>
            <tr><th>Feature</th><th>Tree</th><th>Graph</th></tr>
            <tr><td>Definition</td><td>A hierarchical data structure with nodes connected by edges.</td><td>A general data structure with nodes (vertices) connected by edges (can be directed/undirected).</td></tr>
            <tr><td>Structure</td><td>Connected and acyclic.</td><td>May be connected or disconnected, cyclic or acyclic.</td></tr>
            <tr><td>Root</td><td>Has a single root node.</td><td>No concept of a root (unless it's a tree-based graph).</td></tr>
            <tr><td>Parent-Child Relationship</td><td>Nodes follow a strict parent-child hierarchy.</td><td>No strict hierarchy; any node can connect to any other node.</td></tr>
            <tr><td>Edges</td><td>Exactly n-1 edges for n nodes.</td><td>Can have any number of edges (up to n(n-1)/2 for undirected graphs).</td></tr>
            <tr><td>Traversal</td><td>DFS and BFS, plus tree-specific traversals (inorder, preorder, postorder).</td><td>Typically DFS and BFS, without hierarchy-based traversal.</td></tr>
            <tr><td>Cyclic Nature</td><td>Always acyclic.</td><td>Can contain cycles (e.g., directed graphs with loops).</td></tr>
        </table>

        <h3>Tree Traversals</h3>
        <ul>
            <li><strong>Preorder:</strong> Visit root → Left subtree → Right subtree.</li>
            <li><strong>Inorder:</strong> Visit Left subtree → Root → Right subtree (used in BSTs for sorted order).</li>
            <li><strong>Postorder:</strong> Visit Left subtree → Right subtree → Root.</li>
            <li><strong>Level-order:</strong> Breadth-first traversal level by level.</li>
        </ul>

        <h3>Graph Traversals</h3>
        <ul>
            <li><strong>Depth First Search (DFS)</strong></li>
            <li><strong>Breadth First Search (BFS)</strong></li>
        </ul>

        <h2>7. Sorting and Searching Algorithms</h2>
        <table>
            <tr><th>Algorithm</th><th>Technique</th><th>Time Complexity</th><th>Real-World Applications</th></tr>
            <tr><td>Bubble Sort</td><td>Compare adjacent elements and swap if needed.</td><td>O(n²)</td><td>Educational purposes to teach algorithm basics.</td></tr>
            <tr><td>Selection Sort</td><td>Find the minimum and place it at the beginning.</td><td>O(n²)</td><td>Simple applications where memory is minimal (e.g., small embedded systems).</td></tr>
            <tr><td>Insertion Sort</td><td>Build a sorted portion by inserting elements.</td><td>O(n²)</td><td>Small datasets like sorting playing cards.</td></tr>
            <tr><td>Merge Sort</td><td>Divide-and-conquer: split, sort, and merge.</td><td>O(n log n)</td><td>Sorting large datasets in external storage, like disk drives.</td></tr>
            <tr><td>Quick Sort</td><td>Partitioning based on a pivot element.</td><td>O(n log n)</td><td>Databases, language libraries (e.g., Python's Timsort).</td></tr>
            <tr><td>Heap Sort</td><td>Use a heap data structure to extract max/min.</td><td>O(n log n)</td><td>Scheduling systems and prioritization.</td></tr>
        </table>

        <h2>8. Importance of Graph Algorithms</h2>
        <ul>
            <li><strong>Dijkstra’s Algorithm:</strong> Finds the shortest path from a single source to all other vertices in a weighted graph with non-negative weights. Applications: Roadmaps, telecommunications.</li>
            <li><strong>Floyd’s Algorithm:</strong> Computes shortest paths between all pairs of vertices in a weighted graph. Applications: Urban transit planning.</li>
            <li><strong>Warshall’s Algorithm:</strong> Determines transitive closure to verify connectivity before spanning tree construction.</li>
            <li><strong>Kruskal’s Algorithm:</strong> Constructs a minimum spanning tree by sorting and adding edges in order of increasing weight. Applications: Power grids, telecommunication lines.</li>
            <li><strong>Prim’s Algorithm:</strong> Builds a minimum spanning tree by growing one vertex at a time. Efficient for dense graphs.</li>
        </ul>

        <h2>9. Algorithm Design Techniques</h2>
        <ul>
            <li><strong>Divide and Conquer:</strong> Split the problem into smaller subproblems, solve, and merge the solutions. Examples: Merge Sort, Quick Sort, Binary Search.</li>
            <li><strong>Greedy Algorithm:</strong> Make locally optimal choices at each step. Examples: Prim’s and Kruskal’s Algorithms.</li>
            <li><strong>Backtracking:</strong> Explore all possible solutions incrementally and abandon paths that fail constraints. Examples: Sudoku, N-Queens Problem.</li>
            <li><strong>Recursion:</strong> Solve a problem by solving smaller instances of the same problem. Examples: Tower of Hanoi, Fibonacci numbers.</li>
            <li><strong>Brute Force:</strong> Try all possible solutions and select the best one.</li>
        </ul>
    </div>
</body>

